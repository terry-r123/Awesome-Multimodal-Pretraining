# Awesome Captioning:[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<p align="center">
  <img width="250" src="https://camo.githubusercontent.com/1131548cf666e1150ebd2a52f44776d539f06324/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f6d61737465722f6d656469612f6c6f676f2e737667" "Awesome!">
</p>

A curated list of **VL Pretrained Models** and related area. 



## Table of Contents
  * [Papers](#papers)
    * [2021](#2021)
        - [ACL 2021](#ACL-2021)
        - [IJCAI 2021](#IJCAI-2021)
        - [NAACL 2021](#NAACL-2021)
        - [CVPR 2021](#CVPR-2021)
        - [AAAI 2021](#AAAI-2021)
     * [2020](#2020)
        - [EMNLP 2020](#EMNLP-2020)
        - [NIPS 2020](#NIPS-2020)
        - [ACM MM 2020](#ACMMM-2020)
        - [ECCV 2020](#ECCV-2020)
        - [IJCAI 2020](#IJCAI-2020)
        - [ACL 2020](#ACL-2020)
        - [CVPR 2020](#CVPR-2020)
        - [AAAI 2020](#AAAI-2020)
     * [2019](#2019)
        * [NIPS 2019](#NIPS-2019)
        * [ICCV 2019](#ICCV-2019)
        * [ACL 2019](#ACL-2019)
        * [CVPR 2019](#CVPR-2019)
        * [AAAI 2019](#AAAI-2019)
     * [2018](#2018)
        * [NIPS 2018](#NIPS-2018)
        * [ECCV 2018](#ECCV-2018)
        * [ACL 2018](#ACL-2018)
        * [CVPR 2018](#CVPR-2018)
     * [2015](#2015)
        * [ICML 2015](#ICML-2015)
        * [CVPR 2015](#CVPR-2015)
        * [ICLR 2015](#ICLR-2015)
  * [Reference and Acknowledgement](#reference-and-acknowledgement)
## Survey Papers
### 2022
####arxiv 2022
- VLP: A Survey on Vision-Language Pre-training. [[paper]](https://arxiv.org/pdf/2202.09061.pdf)
- A Survey of Vision-Language Pre-Trained Models. [[paper]](https://arxiv.org/pdf/2202.10936.pdf)
## Papers

### 2022
####arxiv 2022
- Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. [[paper]](https://arxiv.org/pdf/2201.07207.pdf)
- data2vec: A General Framework for Self-supervised Learning in Speech, Vision
and Language. [[paper]](https://scontent-hkg4-1.xx.fbcdn.net/v/t39.8562-6/271974914_483120576492438_4239522333319653600_n.pdf?_nc_cat=107&ccb=1-5&_nc_sid=ae5e01&_nc_ohc=HLSTIdOnYI4AX8EOgQJ&_nc_ht=scontent-hkg4-1.xx&oh=00_AT8L5iMi-4myIh4JOfAuOEK3dNlTLSN4BpYF9J-ANtyRGQ&oe=61F00351)
- CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks. [[paper]](https://arxiv.org/pdf/2201.05729.pdf)
- BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. [[paper]](https://arxiv.org/pdf/2201.12086.pdf) [[code]](https://github.com/salesforce/BLIP)
- MVP: Multi-Stage Vision-Language Pre-Training via Multi-Level Semantic Alignment. [[paper]](https://arxiv.org/pdf/2201.12596.pdf)
- VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training. [[paper]](https://arxiv.org/pdf/2201.12723.pdf)
- Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework. [[paper]](https://arxiv.org/pdf/2202.03052.pdf)
- Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework. [[paper]](https://arxiv.org/pdf/2202.06767.pdf) [[paper]](https://wukong-dataset.github.io/wukong-dataset/)
- HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning. [[paper]](https://arxiv.org/pdf/2203.01311.pdf) [[code]](https://github.com/pliang279/HighMMT)
- Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning. [[paper]](https://arxiv.org/pdf/2203.02053.pdf) [[code]](https://modalitygap.readthedocs.io/en/latest/)
#### CVPR 2022
- Vision-Language Pre-Training with Triple Contrastive Learning. [[paper]](https://arxiv.org/pdf/2202.10401.pdf) [[code]](https://github.com/uta-smile/TCL)
- Multi-modal Alignment using Representation Codebook. [[paper]](https://arxiv.org/pdf/2203.00048.pdf)
- Unsupervised Vision-and-Language Pre-training via Retrieval-based Multi-Granular Alignment. [[paper]](https://arxiv.org/pdf/2203.00242.pdf)
#### ICLR 2022
- How Much Can CLIP Benefit Vision-and-Language Tasks? [[paper]](https://arxiv.org/pdf/2107.06383.pdf) [[code]](https://github.com/clip-vil/CLIP-ViL)
- Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm. [[paper]](https://arxiv.org/pdf/2110.05208.pdf) [[code]](https://github.com/Sense-GVT/)
- Evaluating language-biased image classification based on semantic representations. [[paper]](https://arxiv.org/pdf/2201.11014.pdf)
- SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. [[paper]](https://arxiv.org/pdf/2108.10904.pdf)
- FILIP: Fine-grained Interactive Language-Image Pre-Training. [[paper]](https://openreview.net/pdf?id=cpDhcsEDC2)
### 2021

#### arxiv 2021
- Learning to Prompt for Vision-Language Models. [[paper]](https://arxiv.org/pdf/2109.01134.pdf) [[code]](https://github.com/KaiyangZhou/CoOp)
- CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models. [[paper]](https://arxiv.org/pdf/2109.11797.pdf) 
- NUÂ¨ WA: Visual Synthesis Pre-training for Neural visUal World creAtion. [[paper]](https://arxiv.org/pdf/2111.12417.pdf)
- Prompting Visual-Language Models for Efficient Video Understanding. [[paper]](https://arxiv.org/pdf/2112.04478.pdf) 
- A Fistful of Words: Learning Transferable Visual Models from Bag-of-Words Supervision. [[paper]](https://arxiv.org/pdf/2112.13884.pdf)
- ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation. [[paper]](https://arxiv.org/pdf/2112.15283.pdf)
- Sound and Visual Representation Learning with Multiple Pretraining Tasks. [[paper]](https://arxiv.org/pdf/2201.01046.pdf)
- Self-Training Vision Language BERTs with a Unified Conditional Model. [[paper]](https://arxiv.org/pdf/2201.02010.pdf)
- Distilled Dual-Encoder Model for Vision-Language Understanding. [[paper]](https://arxiv.org/pdf/2112.08723.pdf)
#### NIPS 2021
- Multimodal Few-Shot Learning with Frozen Language Models. [[paper]](https://arxiv.org/pdf/2106.13884.pdf) [[code]](https://fh295.github.io/frozen.html)
- VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer. [[paper]](https://arxiv.org/pdf/2107.02681.pdf) [[code]](https://github.com/zinengtang/VidLanKD)
#### EMNLP 2021
- Data Efficient Masked Language Modeling for Vision and Language. [[paper]](https://arxiv.org/pdf/2109.02040.pdf) (Findings)
- Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers. [[paper]](https://arxiv.org/pdf/2109.04448.pdf)
#### ICCV 2021
- LocTex: Learning Data-Efficient Visual Representations from Localized Textual Supervision. [[paper]](https://arxiv.org/pdf/2108.11950.pdf)
#### ACMMM 2021
- ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration. [[paper]](https://arxiv.org/pdf/2108.07073.pdf)
- Product-oriented Machine Translation with Cross-modal Cross-lingual Pre-training. [[paper]](https://arxiv.org/pdf/2108.11119.pdf) [[code]](https://github.com/syuqings/Fashion-MMT)
- Knowledge Perceived Multi-modal Pretraining in E-commerce. [[paper]](https://arxiv.org/pdf/2109.00895.pdf)
#### ICML 2021
- Learning Transferable Visual Models From Natural Language Supervision. [[paper]](https://arxiv.org/pdf/2103.00020) [[code]](https://github.com/openai/CLIP)
- Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. [[paper]](https://arxiv.org/pdf/2102.05918.pdf)
- Unifying Vision-and-Language Tasks via Text Generation. [[paper]](https://arxiv.org/pdf/2102.02779.pdf) [[code]](https://github.com/j-min/VL-T5)
- ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision. [[paper]](https://arxiv.org/pdf/2102.03334.pdf) [[code]](https://github.com/dandelin/vilt)
#### ACL 2021
- LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding. [[paper]](https://arxiv.org/pdf/2012.14740.pdf)   
- KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation. [[paper]](https://arxiv.org/pdf/2101.00419.pdf)   
- E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning. [[paper]](https://arxiv.org/pdf/2106.01804.pdf)   
- Multi-stage Pre-training over Simplified Multimodal Pre-training Models
[comment]: <> (- VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding.)
- UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning. [[paper]](https://arxiv.org/pdf/2012.15409.pdf) [[code]](https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO)
#### IJCAI 2021
##### Image Captioning
- TCIC: Theme Concepts Learning Cross Language and Vision for Image Captioning. [[paper]](https://arxiv.org/pdf/2106.10936.pdf)   
- UIBert: Learning Generic Multimodal Representations for UI Understanding. [[paper]](https://arxiv.org/pdf/2107.13731.pdf)
#### NAACL 2021
##### Image Captioning
- LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval. [[paper]](https://www.aclweb.org/anthology/2021.naacl-main.77.pdf)   
- Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions. [[paper]](https://www.aclweb.org/anthology/2021.naacl-main.420.pdf)
- Cross-lingual Cross-modal Pretraining for Multimodal Retrieval. [[paper]](https://www.aclweb.org/anthology/2021.naacl-main.285.pdf)
- Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models. [[paper]](https://www.aclweb.org/anthology/2021.naacl-main.195.pdf)

[comment]: <> (- KFU NLP Team at SMM4H 2021 Tasks: Cross-lingual and Cross-modal BERT-based Models for Adverse Drug Effects. [[paper]]&#40;https://www.aclweb.org/anthology/2021.smm4h-1.6.pdf&#41;)

- DeCEMBERT: Learning from Noisy Instructional Videos via Dense Captions and Entropy Minimization. [[paper]](https://www.aclweb.org/anthology/2021.naacl-main.193.pdf)  


#### CVPR 2021
- M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training. [[paper]](https://arxiv.org/pdf/2006.02635.pdf) [[code]](https://github.com/microsoft/M3P)
- VinVL: Revisiting Visual Representations in Vision-Language Models. [[paper]](https://arxiv.org/pdf/2101.00529.pdf) [[code]](https://github.com/pzzhang/VinVL)
#### AAAI 2021
##### Single Stream
- ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph. [[paper]](https://arxiv.org/pdf/2006.16934.pdf)

#### ToMM 2021
- Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular Vision-Language Pre-training. [[paper]](https://arxiv.org/pdf/2201.04026.pdf)
### 2020
#### EMNLP 2020
- Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision. [[paper]](https://arxiv.org/pdf/2010.06775.pdf) [[code]](https://github.com/airsplay/vokenization)

#### NIPS 2020
##### Image Captioning
- RATT: Recurrent Attention to Transient Tasks for Continual Image Captioning.  [[paper]](https://proceedings.neurips.cc/paper/2020/file/c2964caac096f26db222cb325aa267cb-Paper.pdf)  
- Diverse Image Captioning with Context-Object Split Latent Spaces. [[paper]](https://papers.nips.cc/paper/2020/file/24bea84d52e6a1f8025e313c2ffff50a-Paper.pdf)  
- Prophet Attention: Predicting Attention with Future Attention for Improved Image Captioning. [[paper]](https://proceedings.neurips.cc/paper/2020/file/13fe9d84310e77f13a6d184dbf1232f3-Paper.pdf)   

#### ACMMM 2020
##### Image Captioning
- Structural Semantic Adversarial Active Learning for Image Captioning. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413885) 
- Iterative Back Modification for Faster Image Captioning. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413901) 
- Bridging the Gap between Vision and Language Domains for Improved Image Captioning. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3414004) 
- Hierarchical Scene Graph Encoder-Decoder for Image Paragraph Captioning. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413859) 
- Improving Intra- and Inter-Modality Visual Relation for Image Captioning. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413877) 
- ICECAP: Information Concentrated Entity-aware Image Captioning. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413576) 
- Attacking Image Captioning Towards Accuracy-Preserving Target Words Removal. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3414009) 

##### Text Captioning
- Multimodal Attention with Image Text Spatial Relationship for OCR-Based Image Captioning. [[paper]](https://dl.acm.org/doi/pdf/10.1145/3394171.3413753) 

##### Video Captioning
- Controllable Video Captioning with an Exemplar Sentence. [[paper]](https://dl.acm.org/doi/abs/10.1145/3394171.3413908) 
- Poet: Product-oriented Video Captioner for E-commerce. [[paper]](https://arxiv.org/abs/2008.06880)  [[code]](https://github.com/shengyuzhang/Poet)
- Learning Semantic Concepts and Temporal Alignment for Narrated Video Procedural Captioning. [[paper]](https://dl.acm.org/doi/10.1145/3394171.3413498) 
- Relational Graph Learning for Grounded Video Description Generation. [[paper]](https://dl.acm.org/doi/abs/10.1145/3394171.3413746)  

#### ECCV 2020
##### Single-Stream
- Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks. [[paper]](https://arxiv.org/pdf/2004.06165.pdf) [[code]](https://github.com/microsoft/Oscar)
- UNITER: UNiversal Image-TExt Representation Learning. [[paper]](https://arxiv.org/pdf/1909.11740.pdf) [[code]](https://github.com/ChenRocks/UNITER)

#### IJCAI 2020
##### Image Captioning
- Human Consensus-Oriented Image Captioning. [[paper]](https://www.ijcai.org/Proceedings/2020/0092.pdf) 
- Non-Autoregressive Image Captioning with Counterfactuals-Critical Multi-Agent Learning. [[paper]](https://www.ijcai.org/Proceedings/2020/0107.pdf) 
- Recurrent Relational Memory Network for Unsupervised Image Captioning. [[paper]](https://www.ijcai.org/Proceedings/2020/0128.pdf) 
##### Video Captioning
- Learning to Discretely Compose Reasoning Module Networks for Video Captioning. [[paper]](https://arxiv.org/abs/2007.09049)  [[code]](https://github.com/tgc1997/RMN)
- SBAT: Video Captioning with Sparse Boundary-Aware Transformer. [[paper]](https://www.ijcai.org/Proceedings/2020/88) 
- Hierarchical Attention Based Spatial-Temporal Graph-to-Sequence Learning for Grounded Video Description. [[paper]](https://www.ijcai.org/Proceedings/2020/131) 

#### ACL 2020
##### Image Captioning
- Clue: Cross-modal Coherence Modeling for Caption Generation. [[paper]](https://www.aclweb.org/anthology/2020.acl-main.583.pdf) 
- Improving Image Captioning Evaluation by Considering Inter References Variance. [[paper]](https://www.aclweb.org/anthology/2020.acl-main.93.pdf) 
- Improving Image Captioning with Better Use of Caption. [[paper]](https://www.aclweb.org/anthology/2020.acl-main.664.pdf)  [[code]](https://github.com/Gitsamshi/WeakVRD-Captioning)
##### Video Captioning
- MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning. [[paper]](https://www.aclweb.org/anthology/2020.acl-main.233.pdf)  [[code]](https://github.com/jayleicn/recurrent-transformer)

[comment]: <> (#### ICME 2020)

[comment]: <> (##### Image Captioning)

[comment]: <> (- [Fooled by Imagination: Adversarial Attack to Image Captioning Via Perturbation in Complex Domain]&#40;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9102842&#41; - Shaofeng Zhang et al, **ICME 2020**. )

[comment]: <> (- [Modeling Local and Global Contexts for Image Captioning]&#40;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9102935&#41; - Peng Yao et al, **ICME 2020**. )

[comment]: <> (##### Video Captioning)

[comment]: <> (- [Video Captioning With Temporal And Region Graph Convolution Network]&#40;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9102967&#41; - Xinlong Xiao et al, **ICME 2020**. )

#### CVPR 2020
##### Image Captioning
- Context-Aware Group Captioning via Self-Attention and Contrastive Features. [[paper]](https://arxiv.org/abs/2004.03708)  [[code]](https://lizw14.github.io/project/groupcap)
- Show, Edit and Tell: A Framework for Editing Image Captions. [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Sammani_Show_Edit_and_Tell_A_Framework_for_Editing_Image_Captions_CVPR_2020_paper.pdf)  [[code]](https://github.com/fawazsammani/show-edit-tell)
- Say As You Wish: Fine-Grained Control of Image Caption Generation With Abstract Scene Graphs. [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Say_As_You_Wish_Fine-Grained_Control_of_Image_Caption_Generation_CVPR_2020_paper.pdf)  [[code]](https://github.com/cshizhe/asg2cap)
- Normalized and Geometry-Aware Self-Attention Network for Image Captioning. [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Normalized_and_Geometry-Aware_Self-Attention_Network_for_Image_Captioning_CVPR_2020_paper.pdf) 
- Meshed-Memory Transformer for Image Captioning. [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Cornia_Meshed-Memory_Transformer_for_Image_Captioning_CVPR_2020_paper.pdf)  [[code]](https://github.com/aimagelab/meshed-memory-transformer)
- X-Linear Attention Networks for Image Captioning. [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_X-Linear_Attention_Networks_for_Image_Captioning_CVPR_2020_paper.pdf)  [[code]](https://github.com/JDAI-CV/image-captioning)
- Transform and Tell: Entity-Aware News Image Captioning. [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Tran_Transform_and_Tell_Entity-Aware_News_Image_Captioning_CVPR_2020_paper.pdf)  [[code]](https://github.com/alasdairtran/transform-and-tell)
- More Grounded Image Captioning by Distilling Image-Text Matching Model. [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_More_Grounded_Image_Captioning_by_Distilling_Image-Text_Matching_Model_CVPR_2020_paper.pdf)  [[code]](https://github.com/YuanEZhou/Grounded-Image-Captioning)
- Better Captioning With Sequence-Level Exploration. [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Better_Captioning_With_Sequence-Level_Exploration_CVPR_2020_paper.html) 

[comment]: <> (- Alleviating Noisy Data in Image Captioning with Cooperative Distillation. [[paper]]&#40;https://arxiv.org/pdf/2012.11691.pdf&#41; -  Pierre Dognin et al, **CVPRW 2020**. )

##### Video Captioning
- Object Relational Graph With Teacher-Recommended Learning for Video Captioning. [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Object_Relational_Graph_With_Teacher-Recommended_Learning_for_Video_Captioning_CVPR_2020_paper.pdf) 
- Spatio-Temporal Graph for Video Captioning With Knowledge Distillation. [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Spatio-Temporal_Graph_for_Video_Captioning_With_Knowledge_Distillation_CVPR_2020_paper.pdf)  [[code]](https://github.com/StanfordVL/STGraph)
- Better Captioning With Sequence-Level Exploration. [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Better_Captioning_With_Sequence-Level_Exploration_CVPR_2020_paper.html)  
- Syntax-Aware Action Targeting for Video Captioning. [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Zheng_Syntax-Aware_Action_Targeting_for_Video_Captioning_CVPR_2020_paper.html)  [[code]](https://github.com/SydCaption/SAAT)   
- Screencast Tutorial Video Understanding. [[paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Screencast_Tutorial_Video_Understanding_CVPR_2020_paper.pdf)  
#### arxiv 2020
##### Single-Stream
- ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data. [[paper]](https://arxiv.org/pdf/2001.07966.pdf) 
- Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers. [[paper]](https://arxiv.org/pdf/2004.00849.pdf)
#### ICLR 2020
##### Single-Stream
- VL-BERT: Pre-training of Generic Visual-Linguistic Representations. [[paper]](https://arxiv.org/pdf/1908.08530.pdf) [[code]](https://github.com/jackroos/VL-BERT)

#### AAAI 2020
##### Single-Stream
- Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training. [[paper]](https://arxiv.org/pdf/1908.06066.pdf) 
#### arxiv 2019
##### Single-Stream
- VisualBERT: A Simple and Performant Baseline for Vision and Language. [[paper]](https://arxiv.org/pdf/1908.03557.pdf) [[code]](https://github.com/uclanlp/visualbert)

### 2019
#### arxiv 2019
##### Single-Stream
- VisualBERT: A Simple and Performant Baseline for Vision and Language. [[paper]](https://arxiv.org/pdf/1908.03557.pdf) [[code]](https://github.com/uclanlp/visualbert)

#### EMNLP 2019
##### Cross-Stream
- LXMERT: Learning Cross-Modality Encoder Representations from Transformers. [[paper]](https://arxiv.org/pdf/1908.07490.pdf) [[code]](https://github.com/airsplay/lxmert)

#### NIPS 2019
##### Cross-Stream
- ViLBERT: Pretraining Task-Agnostic Vision-linguistic Representations for Vision-and-Language Tasks.  [[paper]](https://arxiv.org/pdf/1908.02265.pdf) [[code]](https://github.com/jiasenlu/vilbert_beta)  

  

#### ICCV 2019
##### Image Captioning

- Robust Change Captioning. [[paper]](https://arxiv.org/abs/1901.02527)  

- Attention on Attention for Image Captioning. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Attention_on_Attention_for_Image_Captioning_ICCV_2019_paper.pdf)  

- Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Ge_Exploring_Overall_Contextual_Information_for_Image_Captioning_in_Human-Like_Cognitive_ICCV_2019_paper.pdf)   
  
- Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Datta_Align2Ground_Weakly_Supervised_Phrase_Grounding_Guided_by_Image-Caption_Alignment_ICCV_2019_paper.pdf)  
  
- Hierarchy Parsing for Image Captioning. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Yao_Hierarchy_Parsing_for_Image_Captioning_ICCV_2019_paper.pdf)  

- Generating Diverse and Descriptive Image Captions Using Visual Paraphrases.  [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Generating_Diverse_and_Descriptive_Image_Captions_Using_Visual_Paraphrases_ICCV_2019_paper.pdf)  

- Learning to Collocate Neural Modules for Image Captioning. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Learning_to_Collocate_Neural_Modules_for_Image_Captioning_ICCV_2019_paper.pdf)  

- Sequential Latent Spaces for Modeling the Intention During Diverse Image Captioning. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Aneja_Sequential_Latent_Spaces_for_Modeling_the_Intention_During_Diverse_Image_ICCV_2019_paper.pdf)   

- Towards Unsupervised Image Captioning With Shared Multimodal Embeddings. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Laina_Towards_Unsupervised_Image_Captioning_With_Shared_Multimodal_Embeddings_ICCV_2019_paper.pdf)   

- Human Attention in Image Captioning: Dataset and Analysis. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/He_Human_Attention_in_Image_Captioning_Dataset_and_Analysis_ICCV_2019_paper.pdf)  
  
- Reflective Decoding Network for Image Captioning. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Ke_Reflective_Decoding_Network_for_Image_Captioning_ICCV_2019_paper.pdf)  
  
- Joint Optimization for Cooperative Image Captioning. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Vered_Joint_Optimization_for_Cooperative_Image_Captioning_ICCV_2019_paper.pdf)   
  
- Watch, Listen and Tell: Multi-Modal Weakly Supervised Dense Event Captioning. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Rahman_Watch_Listen_and_Tell_Multi-Modal_Weakly_Supervised_Dense_Event_Captioning_ICCV_2019_paper.pdf)  
  
- Entangled Transformer for Image Captioning. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.pdf)  
  
- nocaps: novel object captioning at scale. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Agrawal_nocaps_novel_object_captioning_at_scale_ICCV_2019_paper.pdf)  
  
- Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Ye_Cap2Det_Learning_to_Amplify_Weak_Caption_Supervision_for_Object_Detection_ICCV_2019_paper.pdf)  
  
- Unpaired Image Captioning via Scene Graph Alignments. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Gu_Unpaired_Image_Captioning_via_Scene_Graph_Alignments_ICCV_2019_paper.pdf)  
  
- Learning to Caption Images Through a Lifetime by Asking Questions. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Shen_Learning_to_Caption_Images_Through_a_Lifetime_by_Asking_Questions_ICCV_2019_paper.pdf)  
  
##### Video Captioning
- VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_VaTeX_A_Large-Scale_High-Quality_Multilingual_Dataset_for_Video-and-Language_Research_ICCV_2019_paper.pdf) 
  
- Controllable Video Captioning With POS Sequence Guidance Based on Gated Fusion Network. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Controllable_Video_Captioning_With_POS_Sequence_Guidance_Based_on_Gated_ICCV_2019_paper.pdf)  
  
- Joint Syntax Representation Learning and Visual Cue Translation for Video Captioning. [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Hou_Joint_Syntax_Representation_Learning_and_Visual_Cue_Translation_for_Video_ICCV_2019_paper.pdf)

#### ACL 2019 ####
##### Image Captioning
- Informative Image Captioning with External Sources of Information  [[paper]](https://www.aclweb.org/anthology/P19-1650.pdf)  

- Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning  [[paper]](https://www.aclweb.org/anthology/P19-1652.pdf)  
  
- Generating Question Relevant Captions to Aid Visual Question Answering  [[paper]](https://www.aclweb.org/anthology/P19-1348.pdf)  
##### Video Captioning
- Dense Procedure Captioning in Narrated Instructional Videos  [[paper]](https://www.aclweb.org/anthology/P19-1641.pdf)  
  
#### CVPR 2019 ####
##### Image Captioning
- Auto-Encoding Scene Graphs for Image Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Auto-Encoding_Scene_Graphs_for_Image_Captioning_CVPR_2019_paper.pdf) [[code]](https://github.com/fengyang0317/unsupervised_captioning)   
  
- Fast, Diverse and Accurate Image Captioning Guided by Part-Of-Speech  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Deshpande_Fast_Diverse_and_Accurate_Image_Captioning_Guided_by_Part-Of-Speech_CVPR_2019_paper.pdf)   

- Unsupervised Image Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Feng_Unsupervised_Image_Captioning_CVPR_2019_paper.pdf) [[code]](https://github.com/fengyang0317/unsupervised_captioning)   
  
- Adversarial Attack to Image Captioning via Structured Output Learning With Latent Variables  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Xu_Exact_Adversarial_Attack_to_Image_Captioning_via_Structured_Output_Learning_CVPR_2019_paper.pdf)  
  
- Describing like Humans: On Diversity in Image Captioning [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Describing_Like_Humans_On_Diversity_in_Image_Captioning_CVPR_2019_paper.pdf)   
  
- MSCap: Multi-Style Image Captioning With Unpaired Stylized Text  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Guo_MSCap_Multi-Style_Image_Captioning_With_Unpaired_Stylized_Text_CVPR_2019_paper.pdf)   
  
- Leveraging Captioning to Boost Semantics for Salient Object Detection  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_CapSal_Leveraging_Captioning_to_Boost_Semantics_for_Salient_Object_Detection_CVPR_2019_paper.pdf)  [[code]](https://github.com/zhangludl/code-and-dataset-for-CapSal)  
  
- Context and Attribute Grounded Dense Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yin_Context_and_Attribute_Grounded_Dense_Captioning_CVPR_2019_paper.pdf)  
  
- Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Dense_Relational_Captioning_Triple-Stream_Networks_for_Relationship-Based_Captioning_CVPR_2019_paper.pdf)   
  
- Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Cornia_Show_Control_and_Tell_A_Framework_for_Generating_Controllable_and_CVPR_2019_paper.pdf)  
  
- Self-Critical N-step Training for Image Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Gao_Self-Critical_N-Step_Training_for_Image_Captioning_CVPR_2019_paper.pdf)  
  
- Look Back and Predict Forward in Image Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Qin_Look_Back_and_Predict_Forward_in_Image_Captioning_CVPR_2019_paper.pdf)  
  
- Intention Oriented Image Captions with Guiding Objects  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Intention_Oriented_Image_Captions_With_Guiding_Objects_CVPR_2019_paper.pdf)   
  
- Adversarial Semantic Alignment for Improved Image Captions  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Dognin_Adversarial_Semantic_Alignment_for_Improved_Image_Captions_CVPR_2019_paper.pdf)  
  
- Good News, Everyone! Context driven entity-aware captioning for news images  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Biten_Good_News_Everyone_Context_Driven_Entity-Aware_Captioning_for_News_Images_CVPR_2019_paper.pdf)  [[code]](https://github.com/furkanbiten/GoodNews)  
  
- Pointing Novel Objects in Image Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Pointing_Novel_Objects_in_Image_Captioning_CVPR_2019_paper.pdf)   
  
- Engaging Image Captioning via Personality  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Shuster_Engaging_Image_Captioning_via_Personality_CVPR_2019_paper.pdf)  
  
- Intention Oriented Image Captions With Guiding Objects  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zheng_Intention_Oriented_Image_Captions_With_Guiding_Objects_CVPR_2019_paper.pdf)  
  
- Exact Adversarial Attack to Image Captioning via Structured Output Learning With Latent Variables  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Xu_Exact_Adversarial_Attack_to_Image_Captioning_via_Structured_Output_Learning_CVPR_2019_paper.pdf)  
  
- Towards Unsupervised Image Captioning with Shared Multimodal Embeddings. [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zadeh_Social-IQ_A_Question_Answering_Benchmark_for_Artificial_Social_Intelligence_CVPR_2019_paper.pdf) 
##### Video Captioning
-  Streamlined Dense Video Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Mun_Streamlined_Dense_Video_Captioning_CVPR_2019_paper.pdf)  

-  Grounded Video Description  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_Grounded_Video_Description_CVPR_2019_paper.pdf)  

-  Adversarial Inference for Multi-Sentence Video Description  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Park_Adversarial_Inference_for_Multi-Sentence_Video_Description_CVPR_2019_paper.pdf)  
   
-  Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Object-Aware_Aggregation_With_Bidirectional_Temporal_Graph_for_Video_Captioning_CVPR_2019_paper.pdf)  
   
-  Memory-Attended Recurrent Network for Video Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Pei_Memory-Attended_Recurrent_Network_for_Video_Captioning_CVPR_2019_paper.pdf)  
   
-  Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning  [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Aafaq_Spatio-Temporal_Dynamics_and_Semantic_Attribute_Enriched_Visual_Encoding_for_Video_CVPR_2019_paper.pdf)  
   

#### AAAI 2019
##### Image Captioning
- Improving Image Captioning with Conditional Generative Adversarial Nets  [[paper]](https://arxiv.org/pdf/1805.07112.pdf)  
  
- Connecting Language to Images: A Progressive Attention-Guided Network for Simultaneous Image Captioning and Language Grounding  [[paper]](https://www.aaai.org/ojs/index.php/AAAI/article/view/4916)  
  
- Meta Learning for Image Captioning  [[paper]](https://www.aaai.org/ojs/index.php/AAAI/article/view/4883)
- Deliberate Residual based Attention Network for Image Captioning  [[paper]](https://www.aaai.org/ojs/index.php/AAAI/article/view/4845/4718) 
  
- Hierarchical Attention Network for Image Captioning  [[paper]](https://www.aaai.org/ojs/index.php/AAAI/article/view/4924)  
  
- Learning Object Context for Dense Captioning  [[paper]](https://www.aaai.org/ojs/index.php/AAAI/article/view/4886)  
 
##### Video Captioning
- Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning  [[code]](https://github.com/eric-xw/Zero-Shot-Video-Captioning) [[paper]](https://arxiv.org/pdf/1811.02765.pdf)  

- Temporal Deformable Convolutional Encoder-Decoder Networks for Video Captioning   [[paper]](https://arxiv.org/pdf/1905.01077v1.pdf)  

- Fully Convolutional Video Captioning with Coarse-to-Fine and Inherited Attention  [[paper]](https://aaai.org/ojs/index.php/AAAI/article/view/4839)  
  
- Motion Guided Spatial Attention for Video Captioning  [[paper]](http://yugangjiang.info/publication/19AAAI-vidcaptioning.pdf)  

### 2018

#### NIPS 2018 ####
##### Video Captioning
- Weakly Supervised Dense Event Captioning in Videos. [[paper]](https://papers.nips.cc/paper/2018/file/49af6c4e558a7569d80eee2e035e2bd7-Paper.pdf)  [[code]](https://github.com/XgDuan/WSDEC)



#### ECCV 2018 ####
##### Image Captioning
- Unpaired Image Captioning by Language Pivoting. [[paper]](https://arxiv.org/pdf/1803.05526.pdf)  [[code]](https://github.com/gujiuxiang/unpaired_image_captioning)
- Exploring Visual Relationship for Image Captioning. [[paper]](https://arxiv.org/pdf/1809.07041.pdf)  

#### ACL 2018
##### Image Captioning
- Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning. [[paper]](https://arxiv.org/pdf/1712.02051.pdf) 

#### CVPR 2018 ####
##### Image Captioning
- Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering. [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.html)  [[code]](https://github.com/peteanderson80/bottom-up-attention) 
- Neural Baby Talk. [[paper]](https://arxiv.org/pdf/1803.09845.pdf) 

### 2015

#### ICML 2015 ####
##### Image Captioning
- Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. [[paper]](https://arxiv.org/pdf/1502.03044.pdf)   

#### CVPR 2015 ####
##### Image Captioning
- Show and Tell: A Neural Image Caption Generator. [[paper]](https://arxiv.org/pdf/1411.4555.pdf)   

#### ICLR 2015 ####
##### Image Captioning
- Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN). [[paper]](https://arxiv.org/pdf/1412.6632.pdf)   


#### VCR
#### AAAI 2022
- SGEITL: Scene Graph Enhanced Image-Text Learning for Visual Commonsense Reasoning. [[paper]](https://arxiv.org/pdf/2112.08587.pdf)

#### Detection
#### arxiv 2021
- RegionCLIP: Region-based Language-Image Pretraining. [[paper]](https://arxiv.org/pdf/2112.09106.pdf) 
#### Retrieval
#### arxiv 2022
- BridgeFormer: Bridging Video-text Retrieval with Multiple Choice Questions. [[paper]](https://arxiv.org/pdf/2201.04850.pdf)

- ActionCLIP: A New Paradigm for Video Action Recognition. [[paper]](https://arxiv.org/pdf/2109.08472.pdf) [[code]](https://github.com/sallymmx/ActionCLIP)
- CLIP4Caption: CLIP for Video Caption. [[paper]](https://arxiv.org/pdf/2110.06615.pdf)
- CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval. [[paper]](https://arxiv.org/pdf/2104.08860.pdf) [[code]](https://github.com/ArrowLuo/CLIP4Clip)
## Reference and Acknowledgement
* [**awesome-image-captioning**](https://github.com/zhjohnchan/awesome-image-captioning) from [Zhihong Chen](https://github.com/zhjohnchan)

Really appreciate for there contributions in this area.
